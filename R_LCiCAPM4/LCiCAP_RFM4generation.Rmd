---
title: "Training and testing final model M4 in LC-iCAP manuscript"
author: "Mark D'Ascenzo (revised by Jennifer Smith)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_collapsed: false
    toc_float: true
    number_sections: true
    toc_depth: 3
    code_folding: hide
    theme: united
    
---
# Introduction

This script was used to generate model M4 described in the clinical validation section of the manuscript. The model was tested on the blind validation set and used for generating data in Data file 4E and Figure S11 of the manuscript. Features for the model were selected from the gene features in the deployment gene set and smoking status of the patient (current or former). **Note that smoking status is only used as an interaction term and not as a stand alone term in the model.**  Features for M4 were selected by feature reduction to select  36 analytically stable genes. This script includes: 1) Model training by nested cross validation (with 25 seeds and 5 outer folds), 2) Selection of M4 based on performance and 3) generation of M4 predictions on the training set (OOB) and predictions on the blind test set. Analysis of performance on the blind test set (using the predictions and true class labels) was done outside of this script. Note that there is a duplicate sample in the blind validation set (two different serum samples from the same patient at the same blood draw were analyzed separately in the LC-iCAP). All performances of models 3 and 4 were calculated with the to prediction probabilities of the duplicate sample averaged into one prediction. This code is only for model M4, Model M3 is in a separate code.

```{css, echo=FALSE}
body .main-container {
  max-width: 1600px !important;
  width: 1600px !important;
}
body {
  max-width: 1600px !important;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = T,
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	fig.width = 7,
  fig.height = 5
)
```

# Load libraries
```{r}
library(caret)
library(dplyr)
library(pROC)

source('R/rf_nested_cv_mc.R')
```

# Data
```{r}

ex_pos <- readRDS(file = 'data/for_modeling/LCII.035.056.hk3-batch_corrected-clean-pos-3.0.20231016.rds')

stable_genes <- read.csv('data/stable_genes.csv')
stable_genes <- stable_genes[stable_genes$p_vals > 0.004,]

n_seeds <- 25
ntree <- 500
stability_sample_fraction <- 0.75
stability_cutoff <- 0.9
num_subsamples <- 50

ver = "101623.1"

overwrite = FALSE # toggle to true to regenerate res_4.rds (the model set containing Model 4)
```

# Overview 

## QC-based gene filtering

The NanoString deployment gene set was first filtered to retain only stable genes. Rationale: QC analysis identified mis-calibration between two code set lots in the NanoString PlexSet data resulting in a batch effect between training and blind test samples. This analysis was done using technical replicates of DMOG chemical control samples run with both training and test set batches. To overcome the miscalibration, gene filtering was done remove genes with the batch effect retaining only "stable genes".  Stable genes are in "stable_genes.csv".  

## modeling parameters
 * M4 used all stable genes as features and M3 used a feature selection approach to select a subset of the stable genes for modeling (using GLMNet).
 * Performance is assessed using nested-cross-validation.
 * Multiple seed are run (n=25), seed is selected using AUC-q75 (0.75 quantile)
 * Random Forest (RF) is used for model training
 
## Stable Genes
```{r}
stable_genes
```

# 25 Model seeds using ALL stable genes as features (with smoking status as an interaction term)

```{r}

ex <- ex_pos[,ex_pos$project_group != 4]
ex$status <- droplevels(ex$unblind)

# filepath out
fp <- 'results/res_4.rds'
overwrite <- TRUE  # ensure this exists upstream
seed_list <- 1:n_seeds  # make explicit; you can also hard-code a vector if you prefer


if (!overwrite && file.exists(fp)) {
  res_4 <- readRDS(fp)
} else {
  res_list <- lapply(seed_list, function(seed) {
    message(sprintf("Running seed %s", seed))
    rf_nestedcv_stable(
      ex[stable_genes$gene_name,],
      form = status ~ . + .:smoking_status - smoking_status,   # interactions only
      num_subsamples = num_subsamples,
      stability_sample_fraction = stability_sample_fraction,
      stability_cutoff = 0,
      seed = seed,
      ntree = 500,
      preprocess_method = NULL
    )
  })
  
  # collect AUCs and pick the 75th percentile run
  aucs <- vapply(res_list, function(x) x$auc, numeric(1))
  q75 <- as.numeric(quantile(aucs, 0.75, type = 7))

  # choose the run closest to the 75th percentile (define a tie-break rule)
  # if tie, pick the lower index for determinism
  idx_75 <- {
    diffs <- abs(aucs - q75)
    cands <- which(diffs == min(diffs))
    min(cands)
  }
  chosen_seed <- seed_list[idx_75]
  
  # Console message
  msg <- sprintf(
    "Chosen seed: %d | AUC: %.4f (closest to 0.75 quantile = %.4f)",
    chosen_seed, aucs[idx_75], q75
  )
  message(msg)
  
  # Save chosen seed + AUC to a text file
  txt_fp <- sub("\\.rds$", "_meta.txt", fp)
  writeLines(c(
    paste("Chosen seed:", chosen_seed),
    paste("Chosen index:", idx_75),
    paste("AUC:", aucs[idx_75]),
    paste("0.75 quantile:", q75)
  ), txt_fp)
  
  meta <- list(
    seed_list     = seed_list,
    aucs          = aucs,
    q75           = q75,
    chosen_index  = idx_75,
    chosen_seed   = seed_list[idx_75],
    method_note   = "Selected model is the run whose AUC is closest to the 75th percentile (type=7). Ties broken by lowest index.",
    session_info  = utils::sessionInfo(),
    timestamp     = Sys.time()
  )
  # Save wrapped object
  res_4 <- list(results = res_list, meta = meta)
  saveRDS(res_4, file = fp)

  # Extract chosen model
  model_4 <- res_4$results[[res_4$meta$chosen_index]]$final_model

}
```


### Model error rates
```{r message=TRUE}
hist(
  lapply(res_4$results, function(x) x$final_model$err.rate |> mean()) |> unlist(),
  main = "histogram of error rates across 25 seeds using all stable gene features"
)
abline(v=lapply(res_4$results, function(x) x$final_model$err.rate |> mean()) |> unlist() |> mean())

hist(
  lapply(res_4$results, function(x) x$auc) |> unlist(),
  main = "HIstogram of AUCs across 25 seeds using all stable genes as features"
)
abline(v=res_4$results[[which.min(abs(aucs - q75))]]$auc)
```


### Variable importance scores for selected model M4 

M4 was selected for validation as the model with AUC at 75th percentile value
```{r message=TRUE}
varImp(model_4)
```

## Predictions

Model was selected for prediction against the blind test set

```{r}
make_predictions <- function(fp, ex, name, ver = 231016) {
  # Load wrapped results
  res <- readRDS(fp)
  if (is.null(res$results) || is.null(res$meta$chosen_index)) {
    stop("Expected new format: list(results=..., meta=list(chosen_index=..., chosen_seed=...)).")
  }

  runs <- res$results
  idx  <- res$meta$chosen_index
  if (idx < 1 || idx > length(runs)) stop("meta$chosen_index out of bounds.")
  model <- runs[[idx]]$final_model

  # External validation subset
  ex_ev <- ex[, ex$partition_new == "external_validation"]

  # Build EV design matrix to match training formula (~ . * smoking_status)
  ev <- data.frame(t(SummarizedExperiment::assay(ex_ev, "log2_normalized_counts")))
  ev$smoking_status <- SummarizedExperiment::colData(ex_ev)$smoking_status
  mm <- stats::model.matrix(~ . * smoking_status, ev)
  colnames(mm) <- gsub(":", ".", colnames(mm), fixed = TRUE)

  # Get predictor names
  xvars <- model$forest$xvar.names
  if (is.null(xvars) || !length(xvars)) {
    xvars <- rownames(randomForest::importance(model))
  }

  # If none match, try using gene symbols from rowData(ex_ev)
  if (sum(xvars %in% colnames(mm)) == 0 &&
      "gene_name" %in% colnames(SummarizedExperiment::rowData(ex_ev))) {
    m <- SummarizedExperiment::assay(ex_ev, "log2_normalized_counts")
    rn <- as.character(SummarizedExperiment::rowData(ex_ev)$gene_name)
    ok <- !is.na(rn) & nzchar(rn)
    m <- m[ok, , drop = FALSE]
    rownames(m) <- rn[ok]

    ev <- data.frame(t(m))
    ev$smoking_status <- ex_ev$smoking_status
    mm <- stats::model.matrix(~ . * smoking_status, ev)
    colnames(mm) <- gsub(":", ".", colnames(mm), fixed = TRUE)
  }

  # Align: add zero cols for missing, reorder to match model
  present <- intersect(colnames(mm), xvars)
  missing <- setdiff(xvars, present)
  if (length(missing)) {
    message("Adding zero columns for missing predictors: ",
            paste(head(missing, 10), collapse = ", "),
            if (length(missing) > 10) " ..." else "")
    add <- matrix(0, nrow(mm), length(missing), dimnames = list(NULL, missing))
    mm <- cbind(mm[, present, drop = FALSE], add)
  } else {
    mm <- mm[, present, drop = FALSE]
  }
  mm <- mm[, xvars, drop = FALSE]

  # Predict probabilities and label
  prob <- predict(model, as.data.frame(mm), type = "prob")
  preds <- tibble::as_tibble(prob, rownames = "precyte_uid") |>
    dplyr::select(-benign) |>
    dplyr::mutate(pred = dplyr::if_else(malignant > 0.5, "malignant", "benign")) |>
    dplyr::left_join(
      tibble::as_tibble(SummarizedExperiment::colData(ex_ev)) |>
        dplyr::select(precyte_uid, barcode),
      by = "precyte_uid"
    ) |>
    dplyr::relocate(barcode, .after = "precyte_uid")

  # Outputs
  out_dir <- "results/preds"
  if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE)
  readr::write_csv(preds, file.path(out_dir, sprintf("%s_%s_blind_validation_predictions2.csv", ver, name)))

  feat <- randomForest::importance(model) |>
    tibble::as_tibble(rownames = "feature") |>
    dplyr::arrange(dplyr::desc(MeanDecreaseGini))
  readr::write_csv(feat, file.path(out_dir, sprintf("%s_%s_rf_features2.csv", ver, name)))

  # Return a small summary invisibly
  invisible(list(
    preds = preds,
    features = feat,
    chosen_index = idx,
    chosen_seed = res$meta$chosen_seed
  ))
}


```


```{r}
make_predictions('results/res_4.rds', ex_pos, 'model_4')
```

## Preds Histogram

histogram of 80 samples in the bind test set sorted by M4 prediction probabilities (note that these data contain a duplicate sample that was processed twice)
In this analysis, the classes of the blind test set are blinded. Performance of M4 on the blind test set was done outside of this script using unblinded data.

```{r}

hist(
  read_csv("results/preds/231016_model_4_blind_validation_predictions.csv")$malignant,
  main="Model 4: Prediction Probabilities",
  xlab="Prob Malignant"

)

```
### Training votes (prediction probabilities)
```{r}
assertthat::assert_that(all(ex[,rownames(model_4$votes)]$partition_new == "train"))
assertthat::assert_that(all(ex[,ex$partition_new == 'train']$precyte_uid %in% rownames(model_4$votes)))

preds_train = data.frame(malignant=model_4$votes[,2])

write.csv(file='results/preds_train/model_4.csv', preds_train, col.names = F, quote=F)
```
# ROC curve showing Out of bag predictions of M4 on the training set of 96 samples

```{r}

# Merge predicted probabilities with true labels
true_labels <- colData(ex)[rownames(model_4$votes), "status"]
df_roc <- data.frame(
  actual = factor(true_labels, levels = c("benign", "malignant")),
  malignant = model_4$votes[, "malignant"]
)

# Compute ROC and AUC
roc_obj <- roc(df_roc$actual, df_roc$malignant, levels = c("benign", "malignant"), direction = "<")
auc_val <- auc(roc_obj)
auc_ci <- ci.auc(roc_obj)

# draw to the HTML output
plot(roc_obj, col = "#1f78b4", lwd = 2, main = "ROC Curve – Training Set (Model 4, no SS)")
legend("bottomright",
       legend = sprintf("AUC = %.2f (95%% CI: %.2f–%.2f)", auc_val, auc_ci[1], auc_ci[3]),
       bty = "n", text.col = "#1f78b4")
abline(a = 0, b = 1, lty = 2, col = "gray")

# copy the current plot to a PDF
grDevices::dev.copy2pdf(file = "results/preds_train/ROC_Model4_Train.pdf", width = 6, height = 6)
grDevices::dev.off()

sessionInfo()


```





